{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37228a18-0e4f-4584-a292-ab13217f4bf2",
   "metadata": {},
   "source": [
    "# ACCESO API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0c6c57-a4e0-4c1c-9bff-812f64634a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from dateutil import parser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38917b58-a386-4355-b79f-75b9ffd603d5",
   "metadata": {},
   "source": [
    "## 1. CREAMOS API DE ACCESO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d9e8e-4e32-43ad-856b-d30caf38877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"AIisdfjAJFJFAidfjfgnjaiDMMDKKiD\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1943cf0-38b0-43d8-9f79-df36231bf8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "\n",
    "# Creamos la conexión con youtube API\n",
    "youtube = build(\n",
    "    api_service_name, api_version, developerKey=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bcc7c3-5a68-427d-8c6c-9061d285bbf6",
   "metadata": {},
   "source": [
    "### 1.1 INDICAMOS PARAMETROS DE BÚSQUEDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e9814-0937-48ec-a71c-cec1ef035a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICOS = [\n",
    "    \"amnistia\", \n",
    "    \"feminismo\", \n",
    "    \"genero\", \n",
    "    \"monarquia\", \n",
    "    \"salud\", \n",
    "    \"territorio\", \n",
    "    \"violencia_genero\", \n",
    "    \"vivienda\", \n",
    "    \"educacion\", \n",
    "    \"economia\", \n",
    "    \"inmigracion\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718da24-3617-4bdf-8276-3adc232e3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = TOPICOS[3]\n",
    "print(\"Topico elegido\", topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d2730-e5cd-45ca-8684-c5529d8b629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_response = youtube.search().list(\n",
    "    q= f\"Opiniones españolas sobre {topic} reciente\", # q es la query \n",
    "    part=\"id,snippet\",\n",
    "    maxResults= 50, \n",
    "    regionCode='ES', \n",
    "    # pageToken = search_response['nextPageToken']  #pasa de pagina porque apunta a la siguiente respuesta posible\n",
    "    #ejecutar page token 3 veces\n",
    "    \n",
    "  ).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9900e9aa-a38f-40e8-b5f5-7e4537d6be3b",
   "metadata": {},
   "source": [
    "### 1.1.2 EXTRAEMOS INFORMACION DE LOS VIDEOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c8a45e-3c75-422a-b722-de1d210c861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids():\n",
    "    response = []\n",
    "    for video_response in search_response[\"items\"]:\n",
    "        video_id = video_response[\"id\"][\"videoId\"]\n",
    "        response.append(video_id)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b4789-3991-470f-accd-ca830a137099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def get_comments(video_id):\n",
    "    comments = []\n",
    "    try:\n",
    "        response = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            textFormat='plainText'\n",
    "        ).execute()\n",
    "    except Exception:\n",
    "        print(f\"Fallo conseguir comentarios de {video_id}\")\n",
    "        return []\n",
    "\n",
    "    for item in response['items']:\n",
    "        try:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            author_channel_id = item['snippet']['topLevelComment']['snippet']['authorChannelId'][\"value\"]\n",
    "            #author_channel_id = comment['authorChannelId']['value']\n",
    "            comments.append({\"video_id\": video_id, \"author\": author_channel_id, \"comment\": comment})\n",
    "        except KeyError as e:\n",
    "            print(f\"Falta algun valor de un comentario, {e}\")\n",
    "            comments.append(None)\n",
    "            continue\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3a2ca-623d-4c3b-9ff5-5d4c979b7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "comment_video = [] \n",
    "for video in tqdm(video_id): \n",
    "    comments = get_comments(video) \n",
    "    comment_video.extend(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e832c5c9-969c-4b14-a9fe-c8ca155c11d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comentarios_inmigracion= pd.to_csv(f'video_comentarios_{topic}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a4a9b-fe3c-4b28-b116-a43e7358be31",
   "metadata": {},
   "source": [
    "# 2. PROCESAMIENTO DE DATOS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19626c36-5501-466c-b048-a2de5830be53",
   "metadata": {},
   "source": [
    "## 2.1 PREPOCESAMIENTO DE DATOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c468e6a-ad37-4829-bcfc-874662e8d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valores_nulos = comentarios_inmigracion.isnull().sum()  # Esto devuelve la cantidad de valores nulos por columna\n",
    "print(valores_nulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d5b566-e8c8-4c04-adaf-c9cdcf609029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "comentarios_inmigracion.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19074dd-f2d8-45de-a68f-f3c71825e116",
   "metadata": {},
   "source": [
    "## 2.2 TOKENIZACION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e42a3-c27e-4e26-8235-221d40542ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_tokenize todo el comentario\n",
    "comment_st = nltk.tokenize.sent_tokenize(comment)\n",
    "len(comment_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b0ba93-a41e-45d9-8397-7d130516f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_tokenize tokeniza palabra por palabra\n",
    "comment_wt = nltk.tokenize.word_tokenize(comment)\n",
    "len(comment_wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4514e7-55b5-438b-a4b6-758e8c235ad2",
   "metadata": {},
   "source": [
    "### 2.2.1 NORMALIZACION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79754c-14d3-42b2-9ca6-2472bec12214",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "stopwords = [palabra for palabra in stopwords if 'not' not in palabra]\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc730d7a-eb43-4b68-8ff1-55e83e4b7f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_wt_sin_sw = [word for word in comment_wt if word not in stopwords]\n",
    "len(comment_wt_sin_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d4a9f5-5450-487e-9454-934c584ebc2b",
   "metadata": {},
   "source": [
    "### 2.2.2 FRECUENCIA DE PALABRAS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05b305-d428-4b91-819d-d3903fbc9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(comment_wt_sin_sw)\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf743cb-e6ba-4d30-b1ad-82fa16e000c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_comment= pd.DataFrame(list(freq.items()), columns = [\"Word\",\"Frequency\"])\n",
    "freq_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be28a89-5015-445f-b622-b72e63e08b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZACION DE FRECUENCIA\n",
    "plt.figure(figsize = (15,8))\n",
    "plot = sns.barplot(x  = freq_comment.iloc[:30].Word, y = freq_comment.iloc[:30].Frequency)\n",
    "for item in plot.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3cbb7-c3fb-433b-b684-f823e9bf1d42",
   "metadata": {},
   "source": [
    "## 2.3 STEMIZACION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a989615-384c-4d09-acda-ecbeeaf6142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos esta libreria que nos permite reemplzar caracteres\n",
    "import re\n",
    "\n",
    "# Importamos la función que nos permite Stemmizar de nltk y definimos el stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Traemos nuevamente las stopwords\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "#spanish_stopwords.remove('no')\n",
    "#stopword.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408a131-713a-4edb-9e59-da621e1107b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorremos todos los comentarios y le vamos aplicando la Normalizacion y luego el Stemming a cada uno\n",
    "comment_list=[] \n",
    "\n",
    "for comment in comentarios_inmigracion.comment:\n",
    "    # Vamos a reemplzar los caracteres que no sean letras por espacios, limpiando los numeros \n",
    "    comment=re.sub(\"[^a-zA-Z\\s]\",\" \",str(comment)) #original[^a-zA-Z]\n",
    "    # Pasamos todo a minúsculas\n",
    "    comment=comment.lower()\n",
    "    # Remover caracteres especiales\n",
    "    comment = re.sub(r'\\W', ' ', comment)\n",
    "    # Remover múltiples espacios con uno solo\n",
    "    comment = re.sub(r'\\s+', ' ', comment, flags=re.I)\n",
    "    # remover puntuaciones usando expresiones regulares\n",
    "    comment = re.sub(r'[^\\w\\s]', '', comment) \n",
    "    # #remover comas y comillas \n",
    "    # comment = re.sub(comment.replace(',', '').replace(\"'\", ''))\n",
    "    \n",
    "    \n",
    "    # Tokenizar el texto\n",
    "    # Tokenizamos para separar las palabras del comentario \n",
    "    comment=nltk.word_tokenize(comment)\n",
    "    # Eliminamos las palabras de menos de 3 letras\n",
    "    comment= [palabra for palabra in comment if len(palabra)>3]\n",
    "    # Sacamos las Stopwords\n",
    "    comment = [palabra for palabra in comment if not palabra in stopwords]\n",
    "\n",
    "    ## Hasta acá Normalizamos, ahora a stemmizar\n",
    "\n",
    "    # Aplicamos la funcion para buscar la raiz de las palabras\n",
    "    comment=[stemmer.stem(palabra) for palabra in comment]\n",
    "    # Por ultimo volvemos a unir el comentario\n",
    "    comment=\" \".join(comment)\n",
    "\n",
    "    # Vamos armando una lista con todos los comentarios\n",
    "    comment_list.append(comment)\n",
    "    # comentarios_inmigracion[\"comment_normalizado\"] = comment_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724507b3-8f77-4311-b29d-630e0c62f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comentarios_inmigracion[\"comment_stem\"] = comment_list\n",
    "comentarios_inmigracion.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a3da8-36f8-4429-9247-b3a3b34a56fc",
   "metadata": {},
   "source": [
    "### 2.3.1 MODIFICAMOS PARAMETROS DE NORMALIZACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91830c-3811-4a34-9b94-5e2eda64fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorremos todos los comentarios y le vamos aplicando la Normalizacion y luego el Stemming a cada uno\n",
    "comment_list=[] \n",
    "comment_2 = []\n",
    "for comment in comentarios_inmigracion.comment:\n",
    "    # Vamos a reemplzar los caracteres que no sean letras por espacios, limpiando los numeros \n",
    "    comment=re.sub(\"[^a-zA-Z]\",\" \",str(comment))\n",
    "    # Pasamos todo a minúsculas\n",
    "    comment=comment.lower()\n",
    "    # Remover caracteres especiales\n",
    "    # comment = re.sub(r'\\W', ' ', comment)\n",
    "    # Remover múltiples espacios con uno solo\n",
    "    comment = re.sub(r'\\s+', ' ', comment, flags=re.I)\n",
    "    # Tokenizar el texto\n",
    "    # Tokenizamos para separar las palabras del comentario \n",
    "    comment=nltk.word_tokenize(comment)\n",
    "    # Eliminamos las palabras de menos de 3 letras\n",
    "    comment= [palabra for palabra in comment if len(palabra)>3]\n",
    "    # Sacamos las Stopwords\n",
    "    comment = [palabra for palabra in comment if not palabra in stopwords]\n",
    "\n",
    "    ## Hasta acá Normalizamos, ahora a stemmizar\n",
    "\n",
    "    # Aplicamos la funcion para buscar la raiz de las palabras\n",
    "    comment=[stemmer.stem(palabra) for palabra in comment]\n",
    "    # Por ultimo volvemos a unir el comentario\n",
    "    comment=\" \".join(comment)\n",
    "\n",
    "    # Vamos armando una lista con todos los comentarios\n",
    "    comment_2.append(comment)\n",
    "    # comentarios_inmigracion[\"comment_normalizado\"] = comment_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338fa0f3-6c27-4e38-9915-1b73bb6d6e60",
   "metadata": {},
   "source": [
    "### 2.3.2 VECTORIZACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e6ad2-c36d-40d2-9b7b-e8b5c95f2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_comment = list(comentarios_inmigracion['comment_stem'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511036fc-2750-41c0-af2a-fd96b8850785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Usaremos solo las 1000 palabras con mas frecuencia en todo el corpus para generar los vectores\n",
    "max_features=1000\n",
    "# spanish=stopwords.words(\"spanish\")\n",
    "# Es decir que cada instancia tendrá 1000 features\n",
    "cou_vec=CountVectorizer(max_features=max_features, stop_words = spanish_stopwords) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c4db53-9dfc-4320-99ef-29ef3a866194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generarnos los vectores para cada COMENTARIO a partir del corpus total. \n",
    "matriz_titulos = cou_vec.fit_transform(list_comment)\n",
    "\n",
    "# Tomamos las palabras\n",
    "all_words = cou_vec.get_feature_names_out()\n",
    "\n",
    "# Vizualizamos las 50 palabras mas usadas\n",
    "print(\"50 palabras mas usadas: \",all_words[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea98db-86cf-4ab3-8bf9-b2d289004e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transformar el texto en vectores TF-IDF\n",
    "vectores_tfidf = vectorizer.fit_transform(list_comment)\n",
    "\n",
    "# Obtener el tamaño del vocabulario\n",
    "tamaño_vocabulario = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Ver los vectores TF-IDF resultantes \n",
    "print(vectores_tfidf) \n",
    "\n",
    "#output es:  fila 0, la palabra correspondiente al índice 12289 tiene un valor de TF-IDF 0.4093095079403981\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd897c-2452-46d2-abf3-77f5b2477445",
   "metadata": {},
   "source": [
    "### 2.3.3 OTROS PARAMETROS DE VECTORIZACION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634092b-0726-4a94-bd61-161f53c8a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_2 = TfidfVectorizer(ngram_range=(1, 2), max_df=0.5, min_df=2)\n",
    "\n",
    "\n",
    "# Transformar el texto en vectores TF-IDF\n",
    "vectores_tfidf = vectorizer_2.fit_transform(list_comment)\n",
    "\n",
    "# Obtener el tamaño del vocabulario\n",
    "tamaño_vocabulario = len(vectorizer_2.get_feature_names_out())\n",
    "\n",
    "# Ver los vectores TF-IDF resultantes \n",
    "print(vectores_tfidf) \n",
    "#output  fila 0, la palabra correspondiente al índice 18787 tiene un valor de TF-IDF 0.3394112360296132\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b1965-1501-4345-ac84-da53814476be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la matriz dispersa a un array NumPy\n",
    "vectores_tfidf_array = vectores_tfidf.toarray()\n",
    "\n",
    "# Asegurar que el número de filas en vectores_tfidf coincida con la cantidad de filas en comentarios_inmigracion\n",
    "# (vectores_tfidf_array debe tener la misma cantidad de filas que comentarios_inmigracion)\n",
    "if vectores_tfidf_array.shape[0] == len(comentarios_inmigracion):\n",
    "    # Asignar los vectores TF-IDF al DataFrame como una nueva columna\n",
    "    comentarios_inmigracion['vectores_tfidf'] = list(vectores_tfidf_array)\n",
    "else:\n",
    "    print(\"Las longitudes no coinciden entre la matriz dispersa y el DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6e0ea-275c-4a41-a70c-e0cf3390a3a1",
   "metadata": {},
   "source": [
    "## 2.4 LEMMATIZACION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7f8dfd-c206-412c-8b25-81376498ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el lemmatizar de NLTK, y creamos el objeto\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7208e632-4606-41e2-805f-298b39297db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777c5c3-bb0a-4ce2-948b-7fc70157b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('C:/Users/Usuario/Desktop/ID BOOTCAMPS DS/PROGRAMACION/comentarios_inmigracion.csv')\n",
    "# dataset = pd.concat([dataset.headline,dataset.is_sarcastic],axis=1)\n",
    "dataset.dropna(axis=0,inplace=True)\n",
    "\n",
    "# Traemos nuevamente las stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "#stopword.remove('no')\n",
    "#stopword.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa56005-10ab-4312-8b86-3f70cdfc7b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comentarios_list=[]\n",
    "for comentario in comentarios_inmigracion.comment:\n",
    "    # Vamos a reemplzar los caracteres que no sean leras por espacios\n",
    "    comentario=re.sub(\"[^a-zA-Z]\",\" \",str(comentario))\n",
    "    # Pasamos todo a minúsculas\n",
    "    comentario=comentario.lower()\n",
    "    # Tokenizamos para separar las palabras\n",
    "    comentario=nltk.word_tokenize(comentario)\n",
    "\n",
    "    # Aplicamos el Lemmatizer (Esto puede tardar un ratito)\n",
    "    frase_lemma = [wordnet_lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in comentario]\n",
    "\n",
    "\n",
    "    # Eliminamos las palabras d emenos de 3 letras\n",
    "    comentario = [palabra for palabra in comentario if len(palabra)>3]\n",
    "    # Sacamos las Stopwords\n",
    "    comentario = [palabra for palabra in comentario if not palabra in stopwords]\n",
    "\n",
    "    # Por ultimo volvemos a unir el comentario\n",
    "    comentario=\" \".join(comentario)\n",
    "    #comentarios_inmigracion[\"titular_normalizado\"] = comentarios_list\n",
    "    comentarios_list.append(comentario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec4ad1-8aa2-4860-be3f-488ee1bdec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "comentarios_inmigracion[\"comment_lemm\"] = comentarios_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6eacd-c7c0-42df-bd3d-88be43041e9a",
   "metadata": {},
   "source": [
    "# 3. GUARDAMOS CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4f95c-bf35-4d87-ae70-ca5d11e3cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "comentarios_inmigracion.to_csv(f'comentarios_{topic}_procesados.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
